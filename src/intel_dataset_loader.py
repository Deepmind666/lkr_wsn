# Enhanced Intel Labæ•°æ®é›†åŠ è½½æ¨¡å— - ç”¨äºEEHFRåè®®
# æ”¯æŒå¤šç§æ•°æ®æºå’Œå¢å¼ºçš„æ•°æ®é¢„å¤„ç†åŠŸèƒ½

import pandas as pd
import numpy as np
import os
import requests
import gzip
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import io
import time
import warnings
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.impute import KNNImputer
import seaborn as sns

class IntelLabDataLoader:
    """å¢å¼ºç‰ˆIntel Berkeley Research Labæ•°æ®é›†åŠ è½½å™¨

    è¯¥æ•°æ®é›†åŒ…å«54ä¸ªä¼ æ„Ÿå™¨åœ¨2004å¹´2æœˆ28æ—¥è‡³4æœˆ5æ—¥æœŸé—´æ”¶é›†çš„æ•°æ®ï¼Œ
    åŒ…æ‹¬æ¸©åº¦ã€æ¹¿åº¦ã€å…‰ç…§å’Œç”µå‹ç­‰ä¿¡æ¯ã€‚

    æ–°å¢åŠŸèƒ½ï¼š
    - å¤šæºæ•°æ®é›†æ”¯æŒ
    - æ™ºèƒ½æ•°æ®æ¸…æ´—
    - æ•°æ®å¢å¼ºç­–ç•¥
    - æ—¶ç©ºç‰¹å¾å·¥ç¨‹
    """

    def __init__(self, data_dir="../data", use_synthetic=False):
        """åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨

        å‚æ•°:
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•
            use_synthetic: æ˜¯å¦ä½¿ç”¨åˆæˆæ•°æ®ï¼ˆå½“çœŸå®æ•°æ®ä¸å¯ç”¨æ—¶ï¼‰
        """
        self.data_dir = data_dir
        self.use_synthetic = use_synthetic

        # çœŸå®Intel Labæ•°æ®æ–‡ä»¶è·¯å¾„
        self.data_file = os.path.join(data_dir, "data.txt.gz")
        self.locations_file = os.path.join(data_dir, "Intel_Lab_Data", "mote_locs.txt")
        self.connectivity_file = os.path.join(data_dir, "Intel_Lab_Data", "connectivity.txt")

        # æ•°æ®å­˜å‚¨
        self.sensor_data = None
        self.connectivity_data = None
        self.locations_data = None
        self.processed_data = None

        # æ•°æ®é¢„å¤„ç†å‚æ•°
        self.scaler_features = MinMaxScaler()
        self.scaler_targets = MinMaxScaler()
        self.imputer = KNNImputer(n_neighbors=5)

        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        if not os.path.exists(data_dir):
            os.makedirs(data_dir)

        # åˆå§‹åŒ–æ•°æ®
        self._initialize_data()
    
    def _initialize_data(self):
        """åˆå§‹åŒ–æ•°æ®é›†"""
        try:
            # å°è¯•åŠ è½½çœŸå®æ•°æ®
            if os.path.exists(self.data_file) and os.path.getsize(self.data_file) > 1000:
                self.load_real_data()
                print("âœ… æˆåŠŸåŠ è½½çœŸå®Intel Labæ•°æ®é›†")
            else:
                if self.use_synthetic:
                    self.generate_synthetic_data()
                    print("âš ï¸ çœŸå®æ•°æ®ä¸å¯ç”¨ï¼Œå·²ç”Ÿæˆåˆæˆæ•°æ®é›†")
                else:
                    raise FileNotFoundError("çœŸå®æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ä¸”æœªå¯ç”¨åˆæˆæ•°æ®")
        except Exception as e:
            print(f"âŒ æ•°æ®åˆå§‹åŒ–å¤±è´¥: {e}")
            if self.use_synthetic:
                self.generate_synthetic_data()
                print("ğŸ”„ å·²åˆ‡æ¢åˆ°åˆæˆæ•°æ®é›†")

    def load_real_data(self):
        """åŠ è½½çœŸå®çš„Intel Labæ•°æ®é›†"""
        print("ğŸ“‚ æ­£åœ¨åŠ è½½çœŸå®Intel Labæ•°æ®é›†...")

        # åŠ è½½ä¼ æ„Ÿå™¨æ•°æ®
        try:
            # è¯»å–å‹ç¼©çš„ä¼ æ„Ÿå™¨æ•°æ®æ–‡ä»¶
            with gzip.open(self.data_file, 'rt') as f:
                lines = f.readlines()

            # è§£ææ•°æ®
            sensor_data = []
            for line_num, line in enumerate(lines):
                parts = line.strip().split()
                if len(parts) >= 8:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å­—æ®µ
                    try:
                        # Intel Labæ•°æ®æ ¼å¼: date time epoch moteid temperature humidity light voltage
                        date_str = parts[0]
                        time_str = parts[1]
                        epoch = int(parts[2])
                        node_id = int(parts[3])
                        temperature = float(parts[4]) if parts[4] != '-' else np.nan
                        humidity = float(parts[5]) if parts[5] != '-' else np.nan
                        light = float(parts[6]) if parts[6] != '-' else np.nan
                        voltage = float(parts[7]) if parts[7] != '-' else np.nan

                        # åˆ›å»ºæ—¶é—´æˆ³
                        timestamp = datetime.strptime(f"{date_str} {time_str}", "%Y-%m-%d %H:%M:%S.%f")

                        sensor_data.append({
                            'timestamp': timestamp,
                            'epoch': epoch,
                            'node_id': node_id,
                            'temperature': temperature,
                            'humidity': humidity,
                            'light': light,
                            'voltage': voltage
                        })
                    except (ValueError, IndexError) as e:
                        if line_num < 10:  # åªæ‰“å°å‰å‡ è¡Œçš„é”™è¯¯
                            print(f"  è­¦å‘Š: ç¬¬{line_num+1}è¡Œè§£æå¤±è´¥: {line.strip()[:50]}...")
                        continue  # è·³è¿‡æ ¼å¼é”™è¯¯çš„è¡Œ

            self.sensor_data = pd.DataFrame(sensor_data)
            print(f"âœ… ä¼ æ„Ÿå™¨æ•°æ®åŠ è½½å®Œæˆ: {len(self.sensor_data)} æ¡è®°å½•")

        except Exception as e:
            print(f"âŒ ä¼ æ„Ÿå™¨æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise

        # åŠ è½½ä½ç½®æ•°æ®
        try:
            if os.path.exists(self.locations_file):
                locations = []
                with open(self.locations_file, 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        if len(parts) >= 3:
                            try:
                                node_id = int(parts[0])
                                x = float(parts[1])
                                y = float(parts[2])
                                locations.append({'node_id': node_id, 'x': x, 'y': y})
                            except ValueError:
                                continue

                self.locations_data = pd.DataFrame(locations)
                print(f"âœ… ä½ç½®æ•°æ®åŠ è½½å®Œæˆ: {len(self.locations_data)} ä¸ªèŠ‚ç‚¹")
            else:
                print("âš ï¸ ä½ç½®æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ç”Ÿæˆé»˜è®¤ä½ç½®")
                self._generate_default_locations()

        except Exception as e:
            print(f"âŒ ä½ç½®æ•°æ®åŠ è½½å¤±è´¥: {e}")
            self._generate_default_locations()

        # åŠ è½½è¿æ¥æ€§æ•°æ®
        try:
            if os.path.exists(self.connectivity_file):
                connectivity = []
                with open(self.connectivity_file, 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        if len(parts) >= 2:
                            try:
                                node1 = int(parts[0])
                                node2 = int(parts[1])
                                connectivity.append({'node1': node1, 'node2': node2})
                            except ValueError:
                                continue

                self.connectivity_data = pd.DataFrame(connectivity)
                print(f"âœ… è¿æ¥æ€§æ•°æ®åŠ è½½å®Œæˆ: {len(self.connectivity_data)} æ¡è¿æ¥")
            else:
                print("âš ï¸ è¿æ¥æ€§æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ç”Ÿæˆé»˜è®¤è¿æ¥")
                self._generate_connectivity_data()

        except Exception as e:
            print(f"âŒ è¿æ¥æ€§æ•°æ®åŠ è½½å¤±è´¥: {e}")
            self._generate_connectivity_data()

        # åˆå¹¶ä½ç½®ä¿¡æ¯åˆ°ä¼ æ„Ÿå™¨æ•°æ®
        if self.locations_data is not None and not self.locations_data.empty:
            self.sensor_data = self.sensor_data.merge(
                self.locations_data[['node_id', 'x', 'y']],
                on='node_id',
                how='left'
            )

        print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        print(f"   - èŠ‚ç‚¹æ•°é‡: {self.sensor_data['node_id'].nunique()}")
        print(f"   - æ—¶é—´èŒƒå›´: {self.sensor_data['timestamp'].min()} åˆ° {self.sensor_data['timestamp'].max()}")
        print(f"   - æ•°æ®åˆ—: {list(self.sensor_data.columns)}")

    def _generate_default_locations(self):
        """ç”Ÿæˆé»˜è®¤çš„èŠ‚ç‚¹ä½ç½®"""
        unique_nodes = self.sensor_data['node_id'].unique()
        locations = []

        # ç®€å•çš„ç½‘æ ¼å¸ƒå±€
        cols = int(np.ceil(np.sqrt(len(unique_nodes))))
        for i, node_id in enumerate(sorted(unique_nodes)):
            x = (i % cols) * 5.0
            y = (i // cols) * 5.0
            locations.append({'node_id': node_id, 'x': x, 'y': y})

        self.locations_data = pd.DataFrame(locations)

    def generate_synthetic_data(self):
        """ç”Ÿæˆé«˜è´¨é‡çš„åˆæˆWSNæ•°æ®é›†"""
        print("ğŸ”§ æ­£åœ¨ç”ŸæˆåˆæˆIntel Labæ•°æ®é›†...")

        # ç½‘ç»œå‚æ•°
        n_nodes = 54  # Intel Labå®é™…èŠ‚ç‚¹æ•°
        n_days = 36   # æ•°æ®æ”¶é›†å¤©æ•°
        samples_per_hour = 12  # æ¯å°æ—¶é‡‡æ ·æ¬¡æ•°
        total_samples = n_days * 24 * samples_per_hour

        # ç”Ÿæˆæ—¶é—´æˆ³
        start_time = datetime(2004, 2, 28, 0, 0, 0)
        timestamps = [start_time + timedelta(minutes=5*i) for i in range(total_samples)]

        # ç”ŸæˆèŠ‚ç‚¹ä½ç½®ï¼ˆåŸºäºIntel Labå®é™…å¸ƒå±€ï¼‰
        np.random.seed(42)  # ç¡®ä¿å¯é‡å¤æ€§
        locations = self._generate_node_locations(n_nodes)

        # ç”Ÿæˆä¼ æ„Ÿå™¨æ•°æ®
        sensor_data = []
        for i, timestamp in enumerate(timestamps):
            for node_id in range(1, n_nodes + 1):
                # åŸºäºæ—¶é—´å’Œä½ç½®çš„çœŸå®æ„Ÿæ•°æ®ç”Ÿæˆ
                temp, humidity, light, voltage = self._generate_sensor_reading(
                    node_id, timestamp, locations[node_id-1], i
                )

                sensor_data.append({
                    'timestamp': timestamp,
                    'node_id': node_id,
                    'temperature': temp,
                    'humidity': humidity,
                    'light': light,
                    'voltage': voltage,
                    'x': locations[node_id-1][0],
                    'y': locations[node_id-1][1]
                })

        # è½¬æ¢ä¸ºDataFrame
        self.sensor_data = pd.DataFrame(sensor_data)
        self.locations_data = pd.DataFrame(locations, columns=['x', 'y'])
        self.locations_data['node_id'] = range(1, n_nodes + 1)

        # ç”Ÿæˆè¿æ¥æ€§æ•°æ®
        self._generate_connectivity_data()

        print(f"âœ… åˆæˆæ•°æ®é›†ç”Ÿæˆå®Œæˆ: {len(self.sensor_data)} æ¡è®°å½•")

    def _generate_node_locations(self, n_nodes):
        """ç”ŸæˆèŠ‚ç‚¹ä½ç½®ï¼ˆæ¨¡æ‹ŸIntel Labå¸ƒå±€ï¼‰"""
        # Intel Labæ˜¯ä¸€ä¸ªå¤§çº¦31m x 23mçš„å®éªŒå®¤
        locations = []

        # åˆ›å»ºç½‘æ ¼å¸ƒå±€ï¼Œç„¶åæ·»åŠ éšæœºæ‰°åŠ¨
        rows, cols = 6, 9
        for i in range(n_nodes):
            row = i // cols
            col = i % cols

            # åŸºç¡€ç½‘æ ¼ä½ç½®
            x = col * 3.5 + np.random.normal(0, 0.5)  # æ·»åŠ å™ªå£°
            y = row * 3.8 + np.random.normal(0, 0.5)

            # ç¡®ä¿åœ¨å®éªŒå®¤èŒƒå›´å†…
            x = max(0, min(31, x))
            y = max(0, min(23, y))

            locations.append([x, y])

        return locations

    def _generate_sensor_reading(self, node_id, timestamp, location, sample_idx):
        """ç”Ÿæˆå•ä¸ªä¼ æ„Ÿå™¨è¯»æ•°"""
        # åŸºäºæ—¶é—´çš„å‘¨æœŸæ€§å˜åŒ–
        hour = timestamp.hour
        day_of_year = timestamp.timetuple().tm_yday

        # æ¸©åº¦æ¨¡å‹ï¼ˆè€ƒè™‘æ—¥å¤œå˜åŒ–å’Œå­£èŠ‚å˜åŒ–ï¼‰
        base_temp = 20 + 5 * np.sin(2 * np.pi * hour / 24)  # æ—¥å˜åŒ–
        seasonal_temp = 3 * np.sin(2 * np.pi * day_of_year / 365)  # å­£èŠ‚å˜åŒ–
        spatial_temp = (location[0] + location[1]) * 0.1  # ç©ºé—´å˜åŒ–
        noise_temp = np.random.normal(0, 1)
        temperature = base_temp + seasonal_temp + spatial_temp + noise_temp

        # æ¹¿åº¦æ¨¡å‹ï¼ˆä¸æ¸©åº¦è´Ÿç›¸å…³ï¼‰
        base_humidity = 50 - 0.5 * (temperature - 20)
        humidity_noise = np.random.normal(0, 5)
        humidity = max(0, min(100, base_humidity + humidity_noise))

        # å…‰ç…§æ¨¡å‹ï¼ˆç™½å¤©é«˜ï¼Œå¤œæ™šä½ï¼‰
        if 6 <= hour <= 18:  # ç™½å¤©
            base_light = 500 + 300 * np.sin(np.pi * (hour - 6) / 12)
        else:  # å¤œæ™š
            base_light = 50
        light_noise = np.random.normal(0, 50)
        light = max(0, base_light + light_noise)

        # ç”µå‹æ¨¡å‹ï¼ˆéšæ—¶é—´ç¼“æ…¢ä¸‹é™ï¼Œæ¨¡æ‹Ÿç”µæ± æ¶ˆè€—ï¼‰
        base_voltage = 3.0 - 0.0001 * sample_idx  # ç¼“æ…¢ä¸‹é™
        voltage_noise = np.random.normal(0, 0.05)
        voltage = max(2.0, base_voltage + voltage_noise)

        return temperature, humidity, light, voltage

    def _generate_connectivity_data(self):
        """ç”ŸæˆèŠ‚ç‚¹è¿æ¥æ€§æ•°æ®"""
        connectivity = []
        locations = self.locations_data[['x', 'y']].values

        # åŸºäºè·ç¦»çš„è¿æ¥æ€§ï¼ˆé€šä¿¡èŒƒå›´çº¦5ç±³ï¼‰
        comm_range = 5.0

        for i in range(len(locations)):
            for j in range(i + 1, len(locations)):
                distance = np.sqrt(np.sum((locations[i] - locations[j])**2))
                if distance <= comm_range:
                    # æ·»åŠ ä¸€äº›éšæœºæ€§ï¼ˆä¿¡å·è¡°å‡ã€éšœç¢ç‰©ç­‰ï¼‰
                    connection_prob = max(0, 1 - distance / comm_range)
                    if np.random.random() < connection_prob:
                        connectivity.append({
                            'node1': i + 1,
                            'node2': j + 1,
                            'distance': distance,
                            'link_quality': connection_prob
                        })

        self.connectivity_data = pd.DataFrame(connectivity)
        print(f"âœ… ç”Ÿæˆäº† {len(connectivity)} æ¡è¿æ¥è®°å½•")

    def download_dataset(self):
        """ä¸‹è½½Intel Labæ•°æ®é›†"""
        print("æ­£åœ¨ä¸‹è½½Intel Labæ•°æ®é›†...")
        
        # ä¸‹è½½ä¼ æ„Ÿå™¨æ•°æ®
        if not os.path.exists(self.data_file):
            print(f"ä¸‹è½½ä¼ æ„Ÿå™¨æ•°æ®: {self.data_url}")
            try:
                response = requests.get(self.data_url, stream=True)
                with open(self.data_file, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                print("ä¼ æ„Ÿå™¨æ•°æ®ä¸‹è½½å®Œæˆ")
            except Exception as e:
                print(f"ä¸‹è½½ä¼ æ„Ÿå™¨æ•°æ®å¤±è´¥: {e}")
        else:
            print("ä¼ æ„Ÿå™¨æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
        
        # ä¸‹è½½è¿æ¥æ‹“æ‰‘æ•°æ®
        if not os.path.exists(self.topology_file):
            print(f"ä¸‹è½½è¿æ¥æ‹“æ‰‘æ•°æ®: {self.topology_url}")
            try:
                response = requests.get(self.topology_url, stream=True)
                with open(self.topology_file, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                print("è¿æ¥æ‹“æ‰‘æ•°æ®ä¸‹è½½å®Œæˆ")
            except Exception as e:
                print(f"ä¸‹è½½è¿æ¥æ‹“æ‰‘æ•°æ®å¤±è´¥: {e}")
        else:
            print("è¿æ¥æ‹“æ‰‘æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
        
        # ä¸‹è½½èŠ‚ç‚¹ä½ç½®æ•°æ®
        if not os.path.exists(self.locations_file):
            print(f"ä¸‹è½½èŠ‚ç‚¹ä½ç½®æ•°æ®: {self.locations_url}")
            try:
                response = requests.get(self.locations_url)
                with open(self.locations_file, 'w') as f:
                    f.write(response.text)
                print("èŠ‚ç‚¹ä½ç½®æ•°æ®ä¸‹è½½å®Œæˆ")
            except Exception as e:
                print(f"ä¸‹è½½èŠ‚ç‚¹ä½ç½®æ•°æ®å¤±è´¥: {e}")
        else:
            print("èŠ‚ç‚¹ä½ç½®æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
    
    def load_sensor_data(self, sample_size=None):
        """åŠ è½½ä¼ æ„Ÿå™¨æ•°æ®
        
        å‚æ•°:
            sample_size: é‡‡æ ·å¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™åŠ è½½å…¨éƒ¨æ•°æ®
            
        è¿”å›:
            ä¼ æ„Ÿå™¨æ•°æ®DataFrame
        """
        print("åŠ è½½ä¼ æ„Ÿå™¨æ•°æ®...")
        start_time = time.time()
        
        # å®šä¹‰åˆ—å
        columns = ['date', 'time', 'epoch', 'moteid', 'temperature', 'humidity', 'light', 'voltage']
        
        try:
            # ç›´æ¥æ‰“å¼€æ–‡æœ¬æ–‡ä»¶
            if sample_size is not None:
                lines = []
                with open(self.data_file, 'rt') as f:
                    for i, line in enumerate(f):
                        if i >= sample_size:
                            break
                        lines.append(line.strip())
                
                # è§£ææ•°æ®
                data = [line.split() for line in lines]
                self.sensor_data = pd.DataFrame(data, columns=columns)
            else:
                # è¯»å–å…¨éƒ¨æ•°æ®
                self.sensor_data = pd.read_csv(self.data_file, sep=' ', names=columns)
            
            # è½¬æ¢æ•°æ®ç±»å‹
            self.sensor_data['epoch'] = self.sensor_data['epoch'].astype(int)
            self.sensor_data['moteid'] = self.sensor_data['moteid'].astype(int)
            self.sensor_data['temperature'] = self.sensor_data['temperature'].astype(float)
            self.sensor_data['humidity'] = self.sensor_data['humidity'].astype(float)
            self.sensor_data['light'] = self.sensor_data['light'].astype(float)
            self.sensor_data['voltage'] = self.sensor_data['voltage'].astype(float)
            
            # åˆå¹¶æ—¥æœŸå’Œæ—¶é—´åˆ—
            self.sensor_data['timestamp'] = pd.to_datetime(self.sensor_data['date'] + ' ' + self.sensor_data['time'])
            
            # åˆ é™¤åŸå§‹æ—¥æœŸå’Œæ—¶é—´åˆ—
            self.sensor_data = self.sensor_data.drop(['date', 'time'], axis=1)
            
            print(f"ä¼ æ„Ÿå™¨æ•°æ®åŠ è½½å®Œæˆï¼Œå…±{len(self.sensor_data)}æ¡è®°å½•ï¼Œè€—æ—¶{time.time() - start_time:.2f}ç§’")
            return self.sensor_data
            
        except Exception as e:
            print(f"åŠ è½½ä¼ æ„Ÿå™¨æ•°æ®å¤±è´¥: {e}")
            return None
    
    def load_connectivity_data(self):
        """åŠ è½½è¿æ¥æ‹“æ‰‘æ•°æ®
        
        è¿”å›:
            è¿æ¥æ‹“æ‰‘æ•°æ®DataFrame
        """
        print("åŠ è½½è¿æ¥æ‹“æ‰‘æ•°æ®...")
        
        try:
            # ç›´æ¥æ‰“å¼€æ–‡æœ¬æ–‡ä»¶
            self.connectivity_data = pd.read_csv(self.topology_file, sep=' ', names=['sender', 'receiver', 'probability'])
            
            # è½¬æ¢æ•°æ®ç±»å‹
            self.connectivity_data['sender'] = self.connectivity_data['sender'].astype(int)
            self.connectivity_data['receiver'] = self.connectivity_data['receiver'].astype(int)
            self.connectivity_data['probability'] = self.connectivity_data['probability'].astype(float)
            
            print(f"è¿æ¥æ‹“æ‰‘æ•°æ®åŠ è½½å®Œæˆï¼Œå…±{len(self.connectivity_data)}æ¡è®°å½•")
            return self.connectivity_data
            
        except Exception as e:
            print(f"åŠ è½½è¿æ¥æ‹“æ‰‘æ•°æ®å¤±è´¥: {e}")
            return None
    
    def load_locations_data(self):
        """åŠ è½½èŠ‚ç‚¹ä½ç½®æ•°æ®
        
        è¿”å›:
            èŠ‚ç‚¹ä½ç½®æ•°æ®DataFrame
        """
        print("åŠ è½½èŠ‚ç‚¹ä½ç½®æ•°æ®...")
        
        try:
            # è¯»å–æ•°æ®
            self.locations_data = pd.read_csv(self.locations_file, sep=' ', names=['moteid', 'x', 'y'])
            
            # è½¬æ¢æ•°æ®ç±»å‹
            self.locations_data['moteid'] = self.locations_data['moteid'].astype(int)
            self.locations_data['x'] = self.locations_data['x'].astype(float)
            self.locations_data['y'] = self.locations_data['y'].astype(float)
            
            print(f"èŠ‚ç‚¹ä½ç½®æ•°æ®åŠ è½½å®Œæˆï¼Œå…±{len(self.locations_data)}æ¡è®°å½•")
            return self.locations_data
            
        except Exception as e:
            print(f"åŠ è½½èŠ‚ç‚¹ä½ç½®æ•°æ®å¤±è´¥: {e}")
            return None
    
    def get_node_data(self, node_id, start_time=None, end_time=None):
        """è·å–æŒ‡å®šèŠ‚ç‚¹çš„æ•°æ®
        
        å‚æ•°:
            node_id: èŠ‚ç‚¹ID
            start_time: å¼€å§‹æ—¶é—´ï¼Œæ ¼å¼ä¸º'YYYY-MM-DD HH:MM:SS'
            end_time: ç»“æŸæ—¶é—´ï¼Œæ ¼å¼ä¸º'YYYY-MM-DD HH:MM:SS'
            
        è¿”å›:
            èŠ‚ç‚¹æ•°æ®DataFrame
        """
        if self.sensor_data is None:
            self.load_sensor_data()
        
        # ç­›é€‰æŒ‡å®šèŠ‚ç‚¹çš„æ•°æ®
        node_data = self.sensor_data[self.sensor_data['moteid'] == node_id].copy()
        
        # ç­›é€‰æ—¶é—´èŒƒå›´
        if start_time is not None:
            start_dt = pd.to_datetime(start_time)
            node_data = node_data[node_data['timestamp'] >= start_dt]
        
        if end_time is not None:
            end_dt = pd.to_datetime(end_time)
            node_data = node_data[node_data['timestamp'] <= end_dt]
        
        return node_data
    
    def get_link_quality(self, sender_id, receiver_id):
        """è·å–æŒ‡å®šé“¾è·¯çš„è´¨é‡
        
        å‚æ•°:
            sender_id: å‘é€èŠ‚ç‚¹ID
            receiver_id: æ¥æ”¶èŠ‚ç‚¹ID
            
        è¿”å›:
            é“¾è·¯è´¨é‡ï¼ˆæˆåŠŸæ¦‚ç‡ï¼‰
        """
        if self.connectivity_data is None:
            self.load_connectivity_data()
        
        # ç­›é€‰æŒ‡å®šé“¾è·¯çš„æ•°æ®
        link_data = self.connectivity_data[
            (self.connectivity_data['sender'] == sender_id) &
            (self.connectivity_data['receiver'] == receiver_id)
        ]
        
        if len(link_data) > 0:
            return link_data['probability'].values[0]
        else:
            return 0.0
    
    def get_node_location(self, node_id):
        """è·å–æŒ‡å®šèŠ‚ç‚¹çš„ä½ç½®
        
        å‚æ•°:
            node_id: èŠ‚ç‚¹ID
            
        è¿”å›:
            (x, y) åæ ‡å…ƒç»„
        """
        if self.locations_data is None:
            self.load_locations_data()
        
        # ç­›é€‰æŒ‡å®šèŠ‚ç‚¹çš„æ•°æ®
        node_location = self.locations_data[self.locations_data['moteid'] == node_id]
        
        if len(node_location) > 0:
            return (node_location['x'].values[0], node_location['y'].values[0])
        else:
            return (0, 0)
    
    def get_energy_data(self, node_id, start_time=None, end_time=None):
        """è·å–æŒ‡å®šèŠ‚ç‚¹çš„èƒ½é‡æ•°æ®ï¼ˆåŸºäºç”µå‹ï¼‰
        
        å‚æ•°:
            node_id: èŠ‚ç‚¹ID
            start_time: å¼€å§‹æ—¶é—´ï¼Œæ ¼å¼ä¸º'YYYY-MM-DD HH:MM:SS'
            end_time: ç»“æŸæ—¶é—´ï¼Œæ ¼å¼ä¸º'YYYY-MM-DD HH:MM:SS'
            
        è¿”å›:
            èƒ½é‡æ•°æ®Series
        """
        node_data = self.get_node_data(node_id, start_time, end_time)
        
        # ä½¿ç”¨ç”µå‹ä½œä¸ºèƒ½é‡æŒ‡æ ‡
        energy_data = node_data[['timestamp', 'voltage']].copy()
        energy_data = energy_data.set_index('timestamp')
        
        return energy_data
    
    def get_traffic_data(self, node_ids, time_window='1H'):
        """ä¼°è®¡ç½‘ç»œæµé‡æ•°æ®ï¼ˆåŸºäºæ•°æ®åŒ…æ•°é‡ï¼‰
        
        å‚æ•°:
            node_ids: èŠ‚ç‚¹IDåˆ—è¡¨
            time_window: æ—¶é—´çª—å£ï¼Œé»˜è®¤ä¸º1å°æ—¶
            
        è¿”å›:
            æµé‡æ•°æ®DataFrame
        """
        if self.sensor_data is None:
            self.load_sensor_data()
        
        # ç­›é€‰æŒ‡å®šèŠ‚ç‚¹çš„æ•°æ®
        nodes_data = self.sensor_data[self.sensor_data['moteid'].isin(node_ids)].copy()
        
        # æŒ‰æ—¶é—´çª—å£ç»Ÿè®¡æ•°æ®åŒ…æ•°é‡
        traffic_data = nodes_data.groupby(['moteid', pd.Grouper(key='timestamp', freq=time_window)]).size().reset_index(name='packet_count')
        
        return traffic_data
    
    def visualize_network_topology(self, threshold=0.5):
        """å¯è§†åŒ–ç½‘ç»œæ‹“æ‰‘
        
        å‚æ•°:
            threshold: é“¾è·¯è´¨é‡é˜ˆå€¼ï¼Œåªæ˜¾ç¤ºè´¨é‡é«˜äºé˜ˆå€¼çš„é“¾è·¯
        """
        if self.connectivity_data is None:
            self.load_connectivity_data()
        
        if self.locations_data is None:
            self.load_locations_data()
        
        # ç­›é€‰é«˜è´¨é‡é“¾è·¯
        high_quality_links = self.connectivity_data[self.connectivity_data['probability'] >= threshold]
        
        # åˆ›å»ºå›¾å½¢
        plt.figure(figsize=(12, 10))
        
        # ç»˜åˆ¶èŠ‚ç‚¹
        for _, node in self.locations_data.iterrows():
            plt.scatter(node['x'], node['y'], c='blue', s=100)
            plt.text(node['x'] + 0.5, node['y'] + 0.5, f"Node {node['moteid']}")
        
        # ç»˜åˆ¶é“¾è·¯
        for _, link in high_quality_links.iterrows():
            sender_loc = self.get_node_location(link['sender'])
            receiver_loc = self.get_node_location(link['receiver'])
            
            plt.plot([sender_loc[0], receiver_loc[0]], [sender_loc[1], receiver_loc[1]], 'k-', alpha=link['probability'])
        
        plt.title(f"Intel Labç½‘ç»œæ‹“æ‰‘ (é“¾è·¯è´¨é‡ >= {threshold})")
        plt.xlabel("Xåæ ‡ (ç±³)")
        plt.ylabel("Yåæ ‡ (ç±³)")
        plt.grid(True)
        plt.show()
    
    def visualize_sensor_data(self, node_id, feature='temperature', start_time=None, end_time=None):
        """å¯è§†åŒ–ä¼ æ„Ÿå™¨æ•°æ®
        
        å‚æ•°:
            node_id: èŠ‚ç‚¹ID
            feature: ç‰¹å¾åç§°ï¼Œå¯é€‰å€¼ä¸º'temperature', 'humidity', 'light', 'voltage'
            start_time: å¼€å§‹æ—¶é—´ï¼Œæ ¼å¼ä¸º'YYYY-MM-DD HH:MM:SS'
            end_time: ç»“æŸæ—¶é—´ï¼Œæ ¼å¼ä¸º'YYYY-MM-DD HH:MM:SS'
        """
        node_data = self.get_node_data(node_id, start_time, end_time)
        
        if len(node_data) == 0:
            print(f"èŠ‚ç‚¹{node_id}åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ•°æ®")
            return
        
        # åˆ›å»ºå›¾å½¢
        plt.figure(figsize=(12, 6))
        
        # ç»˜åˆ¶æ•°æ®
        plt.plot(node_data['timestamp'], node_data[feature], 'b-')
        
        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        feature_labels = {
            'temperature': 'æ¸©åº¦ (Â°C)',
            'humidity': 'æ¹¿åº¦ (%)',
            'light': 'å…‰ç…§ (Lux)',
            'voltage': 'ç”µå‹ (V)'
        }
        
        plt.title(f"èŠ‚ç‚¹{node_id}çš„{feature_labels.get(feature, feature)}æ•°æ®")
        plt.xlabel("æ—¶é—´")
        plt.ylabel(feature_labels.get(feature, feature))
        plt.grid(True)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()
    
    def prepare_data_for_eehfr(self, num_nodes=54, time_window='1H', features=['temperature', 'humidity', 'light', 'voltage']):
        """å‡†å¤‡ç”¨äºEEHFRåè®®çš„æ•°æ®
        
        å‚æ•°:
            num_nodes: èŠ‚ç‚¹æ•°é‡
            time_window: æ—¶é—´çª—å£ï¼Œé»˜è®¤ä¸º1å°æ—¶
            features: è¦åŒ…å«çš„ç‰¹å¾åˆ—è¡¨
            
        è¿”å›:
            ç”¨äºEEHFRåè®®çš„æ•°æ®å­—å…¸
        """
        if self.sensor_data is None:
            self.load_sensor_data()
        
        if self.connectivity_data is None:
            self.load_connectivity_data()
        
        if self.locations_data is None:
            self.load_locations_data()
        
        # é€‰æ‹©å‰num_nodesä¸ªèŠ‚ç‚¹
        node_ids = self.locations_data['moteid'].unique()[:num_nodes]
        
        # å‡†å¤‡èŠ‚ç‚¹ä½ç½®æ•°æ®
        node_locations = {}
        for node_id in node_ids:
            node_locations[node_id] = self.get_node_location(node_id)
        
        # å‡†å¤‡é“¾è·¯è´¨é‡æ•°æ®
        link_quality = {}
        for sender_id in node_ids:
            for receiver_id in node_ids:
                if sender_id != receiver_id:
                    link_id = f"{sender_id}-{receiver_id}"
                    link_quality[link_id] = self.get_link_quality(sender_id, receiver_id)
        
        # å‡†å¤‡ä¼ æ„Ÿå™¨æ•°æ®
        sensor_data = {}
        for feature in features:
            sensor_data[feature] = {}
            for node_id in node_ids:
                node_feature_data = self.get_node_data(node_id)
                if len(node_feature_data) > 0:
                    # æŒ‰æ—¶é—´çª—å£é‡é‡‡æ ·
                    resampled_data = node_feature_data.set_index('timestamp')[feature].resample(time_window).mean()
                    sensor_data[feature][node_id] = resampled_data
        
        # å‡†å¤‡æµé‡æ•°æ®
        traffic_data = self.get_traffic_data(node_ids, time_window)
        
        # æ„å»ºEEHFRæ•°æ®å­—å…¸
        eehfr_data = {
            'node_locations': node_locations,
            'link_quality': link_quality,
            'sensor_data': sensor_data,
            'traffic_data': traffic_data
        }
        
        return eehfr_data

# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    loader = IntelLabDataLoader()
    
    # åŠ è½½å°æ ·æœ¬æ•°æ®è¿›è¡Œæµ‹è¯•
    sensor_data = loader.load_sensor_data(sample_size=10000)
    connectivity_data = loader.load_connectivity_data()
    locations_data = loader.load_locations_data()
    
    # æ‰“å°æ•°æ®æ ·æœ¬
    print("\nä¼ æ„Ÿå™¨æ•°æ®æ ·æœ¬:")
    print(sensor_data.head())
    
    print("\nè¿æ¥æ‹“æ‰‘æ•°æ®æ ·æœ¬:")
    print(connectivity_data.head())
    
    print("\nèŠ‚ç‚¹ä½ç½®æ•°æ®æ ·æœ¬:")
    print(locations_data.head())
    
    # å¯è§†åŒ–ç½‘ç»œæ‹“æ‰‘
    loader.visualize_network_topology(threshold=0.7)
    
    # å¯è§†åŒ–èŠ‚ç‚¹1çš„æ¸©åº¦æ•°æ®
    loader.visualize_sensor_data(node_id=1, feature='temperature')
    
    # å‡†å¤‡EEHFRæ•°æ®
    eehfr_data = loader.prepare_data_for_eehfr(num_nodes=20)
    print("\nEEHFRæ•°æ®å‡†å¤‡å®Œæˆ")

    # æµ‹è¯•å¢å¼ºç‰ˆæ•°æ®é¢„å¤„ç†
    try:
        enhanced_data = loader.preprocess_data_enhanced(sequence_length=24, prediction_horizon=6)
        print(f"\nå¢å¼ºç‰ˆæ•°æ®é¢„å¤„ç†å®Œæˆ:")
        print(f"  - åºåˆ—æ•°é‡: {enhanced_data['sequences'].shape[0]}")
        print(f"  - åºåˆ—é•¿åº¦: {enhanced_data['sequence_length']}")
        print(f"  - é¢„æµ‹æ­¥é•¿: {enhanced_data['prediction_horizon']}")
        print(f"  - ç‰¹å¾ç»´åº¦: {len(enhanced_data['feature_cols'])}")
    except Exception as e:
        print(f"å¢å¼ºç‰ˆæ•°æ®é¢„å¤„ç†æµ‹è¯•å¤±è´¥: {e}")

# å¢å¼ºç‰ˆæ•°æ®é¢„å¤„ç†æ–¹æ³•ï¼ˆæ·»åŠ åˆ°IntelLabDataLoaderç±»ä¸­ï¼‰
def preprocess_data_enhanced(self, sequence_length=24, prediction_horizon=6):
    """å¢å¼ºç‰ˆæ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹"""
    if self.sensor_data is None:
        raise ValueError("æ•°æ®æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ç›¸å…³åŠ è½½æ–¹æ³•")

    print("ğŸ”§ å¼€å§‹å¢å¼ºç‰ˆæ•°æ®é¢„å¤„ç†...")

    # 1. æ•°æ®æ¸…æ´—å’Œå¼‚å¸¸å€¼å¤„ç†
    self.sensor_data = self._clean_data_enhanced(self.sensor_data)

    # 2. é«˜çº§ç‰¹å¾å·¥ç¨‹
    self.sensor_data = self._advanced_feature_engineering(self.sensor_data)

    # 3. æ—¶ç©ºåºåˆ—æ•°æ®å‡†å¤‡
    processed_data = self._prepare_spatiotemporal_data(
        self.sensor_data, sequence_length, prediction_horizon
    )

    print("âœ… å¢å¼ºç‰ˆæ•°æ®é¢„å¤„ç†å®Œæˆ")
    return processed_data

def _clean_data_enhanced(self, data):
    """å¢å¼ºç‰ˆæ•°æ®æ¸…æ´—"""
    print("  - æ‰§è¡Œå¢å¼ºç‰ˆæ•°æ®æ¸…æ´—...")

    # é‡å‘½ååˆ—ä»¥ä¿æŒä¸€è‡´æ€§
    if 'moteid' in data.columns:
        data = data.rename(columns={'moteid': 'node_id'})

    # ç§»é™¤æ˜æ˜¾çš„å¼‚å¸¸å€¼å’Œä¼ æ„Ÿå™¨æ•…éšœ
    sensor_cols = ['temperature', 'humidity', 'light', 'voltage']

    for col in sensor_cols:
        if col in data.columns:
            # å®šä¹‰åˆç†çš„ä¼ æ„Ÿå™¨èŒƒå›´
            if col == 'temperature':
                valid_range = (-10, 60)  # æ‘„æ°åº¦
            elif col == 'humidity':
                valid_range = (0, 100)   # ç™¾åˆ†æ¯”
            elif col == 'light':
                valid_range = (0, 10000) # Lux
            elif col == 'voltage':
                valid_range = (2.0, 3.5) # ç”µæ± ç”µå‹

            # æ ‡è®°è¶…å‡ºåˆç†èŒƒå›´çš„å€¼
            mask = (data[col] < valid_range[0]) | (data[col] > valid_range[1])
            data.loc[mask, col] = np.nan

            # ä½¿ç”¨æ»‘åŠ¨ä¸­ä½æ•°å¡«å……å¼‚å¸¸å€¼
            data[col] = data.groupby('node_id')[col].transform(
                lambda x: x.fillna(x.rolling(window=5, min_periods=1).median())
            )

    # å¤„ç†æ—¶é—´æˆ³å¼‚å¸¸
    data = data.sort_values(['node_id', 'timestamp']).reset_index(drop=True)

    return data

def _advanced_feature_engineering(self, data):
    """é«˜çº§ç‰¹å¾å·¥ç¨‹"""
    print("  - æ‰§è¡Œé«˜çº§ç‰¹å¾å·¥ç¨‹...")

    # åŸºç¡€æ—¶é—´ç‰¹å¾
    data['hour'] = data['timestamp'].dt.hour
    data['day_of_week'] = data['timestamp'].dt.dayofweek
    data['day_of_year'] = data['timestamp'].dt.dayofyear
    data['month'] = data['timestamp'].dt.month

    # å‘¨æœŸæ€§ç¼–ç 
    data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)
    data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)
    data['day_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)
    data['day_cos'] = np.cos(2 * np.pi * data['day_of_week'] / 7)
    data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)
    data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)

    # ä¼ æ„Ÿå™¨æ•°æ®çš„ç»Ÿè®¡ç‰¹å¾
    sensor_cols = ['temperature', 'humidity', 'light', 'voltage']

    for col in sensor_cols:
        if col in data.columns:
            # æ»‘åŠ¨çª—å£ç»Ÿè®¡ç‰¹å¾
            for window in [3, 6, 12]:
                data[f'{col}_ma_{window}'] = data.groupby('node_id')[col].rolling(
                    window, min_periods=1).mean().reset_index(0, drop=True)
                data[f'{col}_std_{window}'] = data.groupby('node_id')[col].rolling(
                    window, min_periods=1).std().reset_index(0, drop=True)

            # å·®åˆ†ç‰¹å¾ï¼ˆå˜åŒ–ç‡ï¼‰
            data[f'{col}_diff'] = data.groupby('node_id')[col].diff()
            data[f'{col}_diff_2'] = data.groupby('node_id')[f'{col}_diff'].diff()

            # ç›¸å¯¹äºç½‘ç»œå¹³å‡å€¼çš„åå·®
            data[f'{col}_network_mean'] = data.groupby('timestamp')[col].transform('mean')
            data[f'{col}_deviation'] = data[col] - data[f'{col}_network_mean']

    # èƒ½é‡ç›¸å…³ç‰¹å¾
    if 'voltage' in data.columns:
        # ä¼°ç®—å‰©ä½™èƒ½é‡æ¯”ä¾‹
        data['energy_ratio'] = (data['voltage'] - 2.0) / (3.3 - 2.0)
        data['energy_ratio'] = data['energy_ratio'].clip(0, 1)

        # èƒ½é‡æ¶ˆè€—ç‡
        data['energy_consumption_rate'] = -data.groupby('node_id')['voltage'].diff()

    # ç©ºé—´ç‰¹å¾ï¼ˆå¦‚æœæœ‰ä½ç½®ä¿¡æ¯ï¼‰
    if self.locations_data is not None and 'node_id' in data.columns:
        data = data.merge(self.locations_data, on='node_id', how='left')

        if 'x' in data.columns and 'y' in data.columns:
            # åˆ°ç½‘ç»œä¸­å¿ƒçš„è·ç¦»
            center_x, center_y = data['x'].mean(), data['y'].mean()
            data['distance_to_center'] = np.sqrt(
                (data['x'] - center_x)**2 + (data['y'] - center_y)**2
            )

            # èŠ‚ç‚¹å¯†åº¦ç‰¹å¾
            data['node_density'] = self._calculate_node_density(data)

    # å¡«å……NaNå€¼
    data = data.fillna(method='ffill').fillna(method='bfill')

    return data

def _calculate_node_density(self, data, radius=5.0):
    """è®¡ç®—èŠ‚ç‚¹å¯†åº¦ç‰¹å¾"""
    densities = []
    unique_nodes = data[['node_id', 'x', 'y']].drop_duplicates()

    for _, node in unique_nodes.iterrows():
        # è®¡ç®—åœ¨æŒ‡å®šåŠå¾„å†…çš„é‚»å±…èŠ‚ç‚¹æ•°é‡
        distances = np.sqrt(
            (unique_nodes['x'] - node['x'])**2 +
            (unique_nodes['y'] - node['y'])**2
        )
        neighbors = (distances <= radius).sum() - 1  # æ’é™¤è‡ªå·±
        densities.append({'node_id': node['node_id'], 'node_density': neighbors})

    density_df = pd.DataFrame(densities)
    return data.merge(density_df, on='node_id', how='left')['node_density']

def _prepare_spatiotemporal_data(self, data, sequence_length, prediction_horizon):
    """å‡†å¤‡æ—¶ç©ºåºåˆ—æ•°æ®"""
    print("  - å‡†å¤‡æ—¶ç©ºåºåˆ—æ•°æ®...")

    # æŒ‰èŠ‚ç‚¹å’Œæ—¶é—´æ’åº
    data = data.sort_values(['node_id', 'timestamp']).reset_index(drop=True)

    # é€‰æ‹©ç‰¹å¾åˆ—
    feature_cols = [
        'temperature', 'humidity', 'light', 'voltage',
        'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos'
    ]

    # æ·»åŠ èƒ½é‡ç‰¹å¾
    if 'energy_ratio' in data.columns:
        feature_cols.append('energy_ratio')
    if 'energy_consumption_rate' in data.columns:
        feature_cols.append('energy_consumption_rate')

    # æ·»åŠ æ»‘åŠ¨çª—å£ç‰¹å¾
    for col in ['temperature', 'humidity', 'light', 'voltage']:
        if col in data.columns:
            for window in [3, 6, 12]:
                if f'{col}_ma_{window}' in data.columns:
                    feature_cols.append(f'{col}_ma_{window}')
                if f'{col}_std_{window}' in data.columns:
                    feature_cols.append(f'{col}_std_{window}')
            if f'{col}_diff' in data.columns:
                feature_cols.append(f'{col}_diff')

    # æ·»åŠ ç©ºé—´ç‰¹å¾
    if 'x' in data.columns:
        feature_cols.extend(['x', 'y'])
    if 'distance_to_center' in data.columns:
        feature_cols.append('distance_to_center')
    if 'node_density' in data.columns:
        feature_cols.append('node_density')

    # ç¡®ä¿æ‰€æœ‰ç‰¹å¾åˆ—éƒ½å­˜åœ¨
    feature_cols = [col for col in feature_cols if col in data.columns]
    target_cols = ['temperature', 'humidity', 'light', 'voltage']
    target_cols = [col for col in target_cols if col in data.columns]

    print(f"    é€‰æ‹©çš„ç‰¹å¾åˆ—æ•°é‡: {len(feature_cols)}")
    print(f"    ç›®æ ‡åˆ—æ•°é‡: {len(target_cols)}")

    # æ•°æ®å½’ä¸€åŒ–
    features_scaled = self.scaler_features.fit_transform(data[feature_cols])
    targets_scaled = self.scaler_targets.fit_transform(data[target_cols])

    # åˆ›å»ºæ—¶ç©ºåºåˆ—æ ·æœ¬
    sequences = []
    targets = []
    node_ids = []
    timestamps = []

    for node_id in data['node_id'].unique():
        node_mask = data['node_id'] == node_id
        node_data = data[node_mask].reset_index(drop=True)
        node_features = features_scaled[node_mask]
        node_targets = targets_scaled[node_mask]

        for i in range(len(node_data) - sequence_length - prediction_horizon + 1):
            sequences.append(node_features[i:i + sequence_length])
            targets.append(node_targets[i + sequence_length:i + sequence_length + prediction_horizon])
            node_ids.append(node_id)
            timestamps.append(node_data.iloc[i + sequence_length]['timestamp'])

    print(f"    ç”Ÿæˆçš„åºåˆ—æ•°é‡: {len(sequences)}")

    return {
        'sequences': np.array(sequences),
        'targets': np.array(targets),
        'node_ids': np.array(node_ids),
        'timestamps': timestamps,
        'feature_cols': feature_cols,
        'target_cols': target_cols,
        'scaler_features': self.scaler_features,
        'scaler_targets': self.scaler_targets,
        'sequence_length': sequence_length,
        'prediction_horizon': prediction_horizon
    }

# å°†è¿™äº›æ–¹æ³•æ·»åŠ åˆ°IntelLabDataLoaderç±»ä¸­
IntelLabDataLoader.preprocess_data_enhanced = preprocess_data_enhanced
IntelLabDataLoader._clean_data_enhanced = _clean_data_enhanced
IntelLabDataLoader._advanced_feature_engineering = _advanced_feature_engineering
IntelLabDataLoader._calculate_node_density = _calculate_node_density
IntelLabDataLoader._prepare_spatiotemporal_data = _prepare_spatiotemporal_data