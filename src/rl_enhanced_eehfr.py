#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼ºåŒ–å­¦ä¹ å¢å¼ºçš„Enhanced EEHFRåè®® (RL-Enhanced EEHFR)

åŸºäºQ-Learningçš„è‡ªé€‚åº”è·¯ç”±å†³ç­–ï¼š
1. æ™ºèƒ½ä¸‹ä¸€è·³é€‰æ‹©
2. è‡ªé€‚åº”ç¯å¢ƒå­¦ä¹ 
3. å¤šç›®æ ‡å¥–åŠ±å‡½æ•°ä¼˜åŒ–
4. åˆ†å¸ƒå¼å­¦ä¹ æœºåˆ¶

åŸºäº2024å¹´é¡¶çº§æœŸåˆŠML+WSNç ”ç©¶æˆæœ
ä½œè€…: Enhanced EEHFR Research Team
æ—¥æœŸ: 2025-01-31
ç‰ˆæœ¬: 1.0 (Machine Learning Integration)
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import numpy as np
import math
import random
import copy
import time
import pickle
from typing import List, Tuple, Dict, Optional, Any
from dataclasses import dataclass
from collections import defaultdict, deque

# å¯¼å…¥åŸºç¡€ç»„ä»¶
from benchmark_protocols import Node, NetworkConfig
from improved_energy_model import ImprovedEnergyModel, HardwarePlatform

@dataclass
class RLNode(Node):
    """å¼ºåŒ–å­¦ä¹ èŠ‚ç‚¹ç±»"""
    
    # RLç›¸å…³å±æ€§
    q_table: Dict = None
    state_history: List = None
    action_history: List = None
    reward_history: List = None
    
    # å­¦ä¹ å‚æ•°
    learning_rate: float = 0.1
    discount_factor: float = 0.9
    epsilon: float = 0.1
    epsilon_decay: float = 0.995
    min_epsilon: float = 0.01
    
    # æ€§èƒ½ç»Ÿè®¡
    successful_transmissions: int = 0
    failed_transmissions: int = 0
    total_energy_consumed: float = 0.0
    
    def __post_init__(self):
        super().__post_init__()
        if self.q_table is None:
            self.q_table = defaultdict(lambda: defaultdict(float))
        if self.state_history is None:
            self.state_history = deque(maxlen=100)
        if self.action_history is None:
            self.action_history = deque(maxlen=100)
        if self.reward_history is None:
            self.reward_history = deque(maxlen=100)

class QLearningAgent:
    """Q-Learningæ™ºèƒ½ä½“"""
    
    def __init__(self, node_id: int, learning_rate: float = 0.1, 
                 discount_factor: float = 0.9, epsilon: float = 0.1):
        self.node_id = node_id
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = 0.995
        self.min_epsilon = 0.01
        
        # Qè¡¨ï¼šçŠ¶æ€ -> åŠ¨ä½œ -> Qå€¼
        self.q_table = defaultdict(lambda: defaultdict(float))
        
        # ç»éªŒå›æ”¾
        self.experience_buffer = deque(maxlen=1000)
        
        # å­¦ä¹ ç»Ÿè®¡
        self.learning_episodes = 0
        self.total_reward = 0.0
        
    def discretize_state(self, state_features: Tuple[float, ...]) -> Tuple[int, ...]:
        """å°†è¿ç»­çŠ¶æ€ç¦»æ•£åŒ–"""
        discretized = []
        
        for feature in state_features:
            # å°†[0,1]èŒƒå›´çš„ç‰¹å¾ç¦»æ•£åŒ–ä¸º10ä¸ªåŒºé—´
            discrete_value = min(9, int(feature * 10))
            discretized.append(discrete_value)
        
        return tuple(discretized)
    
    def get_q_value(self, state: Tuple, action: int) -> float:
        """è·å–Qå€¼"""
        discrete_state = self.discretize_state(state)
        return self.q_table[discrete_state][action]
    
    def select_action(self, state: Tuple, available_actions: List[int]) -> int:
        """é€‰æ‹©åŠ¨ä½œï¼ˆÎµ-è´ªå©ªç­–ç•¥ï¼‰"""
        if not available_actions:
            return -1
        
        if random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©
            return random.choice(available_actions)
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€é«˜çš„åŠ¨ä½œ
            q_values = [self.get_q_value(state, action) for action in available_actions]
            max_q = max(q_values)
            
            # å¦‚æœæœ‰å¤šä¸ªæœ€ä¼˜åŠ¨ä½œï¼Œéšæœºé€‰æ‹©ä¸€ä¸ª
            best_actions = [action for action, q in zip(available_actions, q_values) if q == max_q]
            return random.choice(best_actions)
    
    def update_q_value(self, state: Tuple, action: int, reward: float, 
                      next_state: Tuple, next_available_actions: List[int]):
        """æ›´æ–°Qå€¼"""
        discrete_state = self.discretize_state(state)
        discrete_next_state = self.discretize_state(next_state)
        
        # å½“å‰Qå€¼
        current_q = self.q_table[discrete_state][action]
        
        # ä¸‹ä¸€çŠ¶æ€çš„æœ€å¤§Qå€¼
        if next_available_actions:
            max_next_q = max([self.q_table[discrete_next_state][a] for a in next_available_actions])
        else:
            max_next_q = 0.0
        
        # Q-Learningæ›´æ–°å…¬å¼
        new_q = current_q + self.learning_rate * (
            reward + self.discount_factor * max_next_q - current_q
        )
        
        self.q_table[discrete_state][action] = new_q
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.total_reward += reward
        self.learning_episodes += 1
        
        # Îµè¡°å‡
        if self.epsilon > self.min_epsilon:
            self.epsilon *= self.epsilon_decay
    
    def add_experience(self, state: Tuple, action: int, reward: float, 
                      next_state: Tuple, done: bool):
        """æ·»åŠ ç»éªŒåˆ°å›æ”¾ç¼“å†²åŒº"""
        experience = (state, action, reward, next_state, done)
        self.experience_buffer.append(experience)
    
    def get_learning_stats(self) -> Dict[str, float]:
        """è·å–å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_episodes': self.learning_episodes,
            'average_reward': self.total_reward / max(1, self.learning_episodes),
            'current_epsilon': self.epsilon,
            'q_table_size': len(self.q_table)
        }

class StateExtractor:
    """çŠ¶æ€ç‰¹å¾æå–å™¨"""
    
    def __init__(self):
        self.feature_names = [
            'normalized_energy',    # å½’ä¸€åŒ–å‰©ä½™èƒ½é‡
            'distance_to_bs',      # åˆ°åŸºç«™çš„å½’ä¸€åŒ–è·ç¦»
            'neighbor_density',    # é‚»å±…å¯†åº¦
            'environment_factor',  # ç¯å¢ƒå› å­
            'load_factor'         # è´Ÿè½½å› å­
        ]
    
    def extract_state(self, node: RLNode, network_info: Dict) -> Tuple[float, ...]:
        """æå–èŠ‚ç‚¹çŠ¶æ€ç‰¹å¾"""
        
        # 1. å½’ä¸€åŒ–å‰©ä½™èƒ½é‡
        normalized_energy = node.current_energy / node.initial_energy
        
        # 2. åˆ°åŸºç«™çš„å½’ä¸€åŒ–è·ç¦»
        bs_distance = math.sqrt(
            (node.x - network_info['bs_x'])**2 + 
            (node.y - network_info['bs_y'])**2
        )
        max_distance = network_info['max_distance']
        distance_to_bs = bs_distance / max_distance
        
        # 3. é‚»å±…å¯†åº¦
        neighbor_count = len([n for n in network_info['all_nodes'] 
                            if n.id != node.id and n.is_alive and 
                            math.sqrt((n.x - node.x)**2 + (n.y - node.y)**2) <= 30])
        max_neighbors = network_info['max_neighbors']
        neighbor_density = neighbor_count / max_neighbors if max_neighbors > 0 else 0
        
        # 4. ç¯å¢ƒå› å­ï¼ˆåŸºäºç¯å¢ƒç±»å‹ï¼‰
        environment_types = ['indoor', 'outdoor_open', 'outdoor_obstructed', 
                           'industrial', 'urban', 'forest']
        env_type = getattr(node, 'environment_type', 'outdoor_open')
        environment_factor = environment_types.index(env_type) / len(environment_types)
        
        # 5. è´Ÿè½½å› å­
        load_factor = min(1.0, getattr(node, 'data_load', 0) / 10.0)
        
        state = (
            normalized_energy,
            distance_to_bs,
            neighbor_density,
            environment_factor,
            load_factor
        )
        
        return state

class RewardCalculator:
    """å¥–åŠ±å‡½æ•°è®¡ç®—å™¨"""
    
    def __init__(self, energy_weight: float = 0.4, success_weight: float = 0.4, 
                 delay_weight: float = 0.2):
        self.energy_weight = energy_weight
        self.success_weight = success_weight
        self.delay_weight = delay_weight
    
    def calculate_reward(self, transmission_result: Dict) -> float:
        """è®¡ç®—å¤šç›®æ ‡å¥–åŠ±å‡½æ•°"""
        
        # 1. èƒ½è€—å¥–åŠ±ï¼ˆè´Ÿå¥–åŠ±ï¼Œé¼“åŠ±ä½èƒ½è€—ï¼‰
        energy_consumed = transmission_result.get('energy_consumed', 0)
        energy_reward = -energy_consumed * 1000  # è½¬æ¢ä¸ºmJ
        
        # 2. æˆåŠŸå¥–åŠ±
        success = transmission_result.get('success', False)
        success_reward = 10.0 if success else -10.0
        
        # 3. å»¶è¿Ÿå¥–åŠ±ï¼ˆè´Ÿå¥–åŠ±ï¼Œé¼“åŠ±ä½å»¶è¿Ÿï¼‰
        delay = transmission_result.get('delay', 0)
        delay_reward = -delay * 0.1
        
        # 4. ç½‘ç»œç”Ÿå­˜æ—¶é—´å¥–åŠ±
        network_lifetime_bonus = transmission_result.get('lifetime_bonus', 0)
        
        # åŠ æƒç»„åˆ
        total_reward = (
            self.energy_weight * energy_reward +
            self.success_weight * success_reward +
            self.delay_weight * delay_reward +
            network_lifetime_bonus
        )
        
        return total_reward

class RLEnhancedEEHFRProtocol:
    """å¼ºåŒ–å­¦ä¹ å¢å¼ºçš„Enhanced EEHFRåè®®"""
    
    def __init__(self, config: NetworkConfig):
        self.config = config
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.energy_model = ImprovedEnergyModel(HardwarePlatform.CC2420_TELOSB)
        self.state_extractor = StateExtractor()
        self.reward_calculator = RewardCalculator()
        
        # ç½‘ç»œçŠ¶æ€
        self.nodes: List[RLNode] = []
        self.rl_agents: Dict[int, QLearningAgent] = {}
        self.current_round = 0
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.round_statistics = []
        self.total_energy_consumed = 0.0
        self.total_packets_sent = 0
        self.total_packets_received = 0
        
        # å­¦ä¹ å‚æ•°
        self.learning_enabled = True
        self.exploration_rounds = 100  # å‰100è½®ä¸»è¦ç”¨äºæ¢ç´¢
        
        # åˆå§‹åŒ–ç½‘ç»œ
        self._initialize_network()
    
    def _initialize_network(self):
        """åˆå§‹åŒ–ç½‘ç»œèŠ‚ç‚¹å’ŒRLæ™ºèƒ½ä½“"""
        
        self.nodes = []
        self.rl_agents = {}
        
        for i in range(self.config.num_nodes):
            x = random.uniform(0, self.config.area_width)
            y = random.uniform(0, self.config.area_height)
            
            node = RLNode(
                id=i,
                x=x,
                y=y,
                initial_energy=self.config.initial_energy,
                current_energy=self.config.initial_energy,
                is_alive=True,
                is_cluster_head=False,
                cluster_id=-1
            )
            self.nodes.append(node)
            
            # ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ›å»ºRLæ™ºèƒ½ä½“
            self.rl_agents[i] = QLearningAgent(
                node_id=i,
                learning_rate=0.1,
                discount_factor=0.9,
                epsilon=0.3 if self.current_round < self.exploration_rounds else 0.1
            )
        
        # åˆå§‹ç°‡å¤´é€‰æ‹©
        self._select_cluster_heads()
        self._form_clusters()
    
    def _get_network_info(self) -> Dict:
        """è·å–ç½‘ç»œä¿¡æ¯"""
        max_distance = math.sqrt(self.config.area_width**2 + self.config.area_height**2)
        max_neighbors = len(self.nodes) - 1
        
        return {
            'bs_x': self.config.base_station_x,
            'bs_y': self.config.base_station_y,
            'max_distance': max_distance,
            'max_neighbors': max_neighbors,
            'all_nodes': self.nodes
        }
    
    def _select_cluster_heads(self):
        """ä½¿ç”¨RLé€‰æ‹©ç°‡å¤´"""
        alive_nodes = [node for node in self.nodes if node.is_alive]
        if len(alive_nodes) < 2:
            return
        
        target_ch_count = max(1, int(len(alive_nodes) * 0.1))
        network_info = self._get_network_info()
        
        # é‡ç½®ç°‡å¤´çŠ¶æ€
        for node in alive_nodes:
            node.is_cluster_head = False
            node.cluster_id = -1
        
        # ä¸ºæ¯ä¸ªèŠ‚ç‚¹è®¡ç®—æˆä¸ºç°‡å¤´çš„Qå€¼
        ch_candidates = []
        
        for node in alive_nodes:
            state = self.state_extractor.extract_state(node, network_info)
            agent = self.rl_agents[node.id]
            
            # åŠ¨ä½œï¼šæˆä¸ºç°‡å¤´(1) æˆ– ä¸æˆä¸ºç°‡å¤´(0)
            q_ch = agent.get_q_value(state, 1)  # æˆä¸ºç°‡å¤´çš„Qå€¼
            q_member = agent.get_q_value(state, 0)  # æˆä¸ºæˆå‘˜çš„Qå€¼
            
            # ç°‡å¤´å€¾å‘æ€§å¾—åˆ†
            ch_preference = q_ch - q_member + node.current_energy / node.initial_energy
            ch_candidates.append((node, ch_preference))
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„èŠ‚ç‚¹ä½œä¸ºç°‡å¤´
        ch_candidates.sort(key=lambda x: x[1], reverse=True)
        selected_chs = [candidate[0] for candidate in ch_candidates[:target_ch_count]]
        
        # è®¾ç½®ç°‡å¤´çŠ¶æ€
        for i, ch in enumerate(selected_chs):
            ch.is_cluster_head = True
            ch.cluster_id = i
    
    def _form_clusters(self):
        """å½¢æˆç°‡ç»“æ„"""
        cluster_heads = [node for node in self.nodes if node.is_cluster_head and node.is_alive]
        member_nodes = [node for node in self.nodes if not node.is_cluster_head and node.is_alive]
        
        # ä¸ºæ¯ä¸ªæˆå‘˜èŠ‚ç‚¹åˆ†é…æœ€è¿‘çš„ç°‡å¤´
        for member in member_nodes:
            if not cluster_heads:
                continue
            
            min_distance = float('inf')
            best_cluster_head = None
            
            for ch in cluster_heads:
                distance = math.sqrt((member.x - ch.x)**2 + (member.y - ch.y)**2)
                if distance < min_distance:
                    min_distance = distance
                    best_cluster_head = ch
            
            if best_cluster_head:
                member.cluster_id = best_cluster_head.cluster_id
    
    def _rl_next_hop_selection(self, sender: RLNode, candidates: List[RLNode]) -> Optional[RLNode]:
        """ä½¿ç”¨RLé€‰æ‹©ä¸‹ä¸€è·³èŠ‚ç‚¹"""
        if not candidates:
            return None
        
        network_info = self._get_network_info()
        state = self.state_extractor.extract_state(sender, network_info)
        agent = self.rl_agents[sender.id]
        
        # å¯ç”¨åŠ¨ä½œï¼šå€™é€‰èŠ‚ç‚¹çš„ID
        available_actions = [node.id for node in candidates]
        
        # ä½¿ç”¨RLæ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ
        selected_action = agent.select_action(state, available_actions)
        
        # æ‰¾åˆ°å¯¹åº”çš„èŠ‚ç‚¹
        selected_node = next((node for node in candidates if node.id == selected_action), None)
        
        # è®°å½•çŠ¶æ€å’ŒåŠ¨ä½œ
        sender.state_history.append(state)
        sender.action_history.append(selected_action)
        
        return selected_node
    
    def _execute_transmission(self, sender: RLNode, receiver: RLNode) -> Dict:
        """æ‰§è¡Œä¼ è¾“å¹¶è¿”å›ç»“æœ"""
        if not sender.is_alive or not receiver.is_alive:
            return {'success': False, 'energy_consumed': 0, 'delay': 0}
        
        # è®¡ç®—ä¼ è¾“è·ç¦»
        distance = math.sqrt((sender.x - receiver.x)**2 + (sender.y - receiver.y)**2)
        
        # è‡ªé€‚åº”åŠŸç‡æ§åˆ¶
        if distance < 20:
            tx_power = -5.0  # -5 dBm
        elif distance < 50:
            tx_power = 0.0   # 0 dBm
        else:
            tx_power = 5.0   # 5 dBm
        
        # è®¡ç®—èƒ½è€—
        tx_energy = self.energy_model.calculate_transmission_energy(
            self.config.packet_size * 8, distance, tx_power
        )
        rx_energy = self.energy_model.calculate_reception_energy(self.config.packet_size * 8)
        
        # æ›´æ–°èƒ½é‡
        sender.current_energy -= tx_energy
        receiver.current_energy -= rx_energy
        
        # æ›´æ–°ç»Ÿè®¡
        sender.total_energy_consumed += tx_energy
        receiver.total_energy_consumed += rx_energy
        
        # æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
        if sender.current_energy <= 0:
            sender.is_alive = False
            sender.current_energy = 0
        if receiver.current_energy <= 0:
            receiver.is_alive = False
            receiver.current_energy = 0
        
        # è®¡ç®—æˆåŠŸç‡ï¼ˆç®€åŒ–æ¨¡å‹ï¼‰
        success_rate = 0.95 - (distance / 100) * 0.1  # è·ç¦»è¶Šè¿œæˆåŠŸç‡è¶Šä½
        success = random.random() < success_rate
        
        # è®¡ç®—å»¶è¿Ÿï¼ˆç®€åŒ–æ¨¡å‹ï¼‰
        delay = distance * 0.001 + random.uniform(0, 0.01)
        
        return {
            'success': success,
            'energy_consumed': tx_energy + rx_energy,
            'delay': delay,
            'distance': distance
        }
    
    def _update_rl_agents(self, transmission_results: List[Dict]):
        """æ›´æ–°RLæ™ºèƒ½ä½“"""
        if not self.learning_enabled:
            return
        
        network_info = self._get_network_info()
        
        for result in transmission_results:
            sender_id = result['sender_id']
            sender = next((node for node in self.nodes if node.id == sender_id), None)
            
            if not sender or len(sender.state_history) < 2:
                continue
            
            # è·å–çŠ¶æ€å’ŒåŠ¨ä½œ
            prev_state = sender.state_history[-2]
            action = sender.action_history[-1]
            current_state = self.state_extractor.extract_state(sender, network_info)
            
            # è®¡ç®—å¥–åŠ±
            reward = self.reward_calculator.calculate_reward(result)
            sender.reward_history.append(reward)
            
            # è·å–å½“å‰å¯ç”¨åŠ¨ä½œ
            alive_neighbors = [node for node in self.nodes 
                             if node.is_alive and node.id != sender.id]
            available_actions = [node.id for node in alive_neighbors]
            
            # æ›´æ–°Qå€¼
            agent = self.rl_agents[sender_id]
            agent.update_q_value(prev_state, action, reward, current_state, available_actions)
            
            # æ›´æ–°æˆåŠŸ/å¤±è´¥ç»Ÿè®¡
            if result['success']:
                sender.successful_transmissions += 1
            else:
                sender.failed_transmissions += 1
    
    def _data_collection_phase(self) -> List[Dict]:
        """æ•°æ®æ”¶é›†é˜¶æ®µ"""
        transmission_results = []
        cluster_heads = [node for node in self.nodes if node.is_cluster_head and node.is_alive]
        
        for ch in cluster_heads:
            cluster_members = [node for node in self.nodes 
                             if node.cluster_id == ch.cluster_id and not node.is_cluster_head and node.is_alive]
            
            for member in cluster_members:
                if member.current_energy > 0:
                    # ä½¿ç”¨RLé€‰æ‹©ä¼ è¾“ç­–ç•¥
                    candidates = [ch]  # ç°‡å†…ä¼ è¾“åªèƒ½é€‰æ‹©ç°‡å¤´
                    selected_receiver = self._rl_next_hop_selection(member, candidates)
                    
                    if selected_receiver:
                        result = self._execute_transmission(member, selected_receiver)
                        result['sender_id'] = member.id
                        result['receiver_id'] = selected_receiver.id
                        result['phase'] = 'data_collection'
                        transmission_results.append(result)
        
        return transmission_results
    
    def _data_forwarding_phase(self) -> List[Dict]:
        """æ•°æ®è½¬å‘é˜¶æ®µ"""
        transmission_results = []
        cluster_heads = [node for node in self.nodes if node.is_cluster_head and node.is_alive]
        
        for ch in cluster_heads:
            if ch.current_energy > 0:
                # ç°‡å¤´å‘åŸºç«™å‘é€æ•°æ®
                # è¿™é‡Œç®€åŒ–ä¸ºç›´æ¥ä¼ è¾“ï¼Œå®é™…å¯ä»¥ä½¿ç”¨RLé€‰æ‹©ä¸­ç»§èŠ‚ç‚¹
                bs_distance = math.sqrt(
                    (ch.x - self.config.base_station_x)**2 + 
                    (ch.y - self.config.base_station_y)**2
                )
                
                # è®¡ç®—ä¼ è¾“èƒ½è€—
                tx_energy = self.energy_model.calculate_transmission_energy(
                    self.config.packet_size * 8, bs_distance, 0.0
                )
                
                ch.current_energy -= tx_energy
                ch.total_energy_consumed += tx_energy
                
                if ch.current_energy <= 0:
                    ch.is_alive = False
                    ch.current_energy = 0
                
                # åŸºç«™ä¼ è¾“æˆåŠŸç‡è¾ƒé«˜
                success = random.random() < 0.98
                delay = bs_distance * 0.001
                
                result = {
                    'sender_id': ch.id,
                    'receiver_id': -1,  # åŸºç«™
                    'success': success,
                    'energy_consumed': tx_energy,
                    'delay': delay,
                    'distance': bs_distance,
                    'phase': 'data_forwarding'
                }
                transmission_results.append(result)
        
        return transmission_results
    
    def _collect_round_statistics(self, round_num: int, transmission_results: List[Dict]):
        """æ”¶é›†è½®æ¬¡ç»Ÿè®¡ä¿¡æ¯"""
        alive_nodes = sum(1 for node in self.nodes if node.is_alive)
        cluster_heads = sum(1 for node in self.nodes if node.is_cluster_head and node.is_alive)
        remaining_energy = sum(node.current_energy for node in self.nodes if node.is_alive)
        
        # ç»Ÿè®¡ä¼ è¾“ç»“æœ
        packets_sent = len(transmission_results)
        packets_received = sum(1 for result in transmission_results if result['success'])
        round_energy_consumed = sum(result['energy_consumed'] for result in transmission_results)
        
        # RLå­¦ä¹ ç»Ÿè®¡
        total_reward = sum(sum(node.reward_history) for node in self.nodes if node.reward_history)
        avg_epsilon = sum(agent.epsilon for agent in self.rl_agents.values()) / len(self.rl_agents)
        
        round_stats = {
            'round': round_num,
            'alive_nodes': alive_nodes,
            'cluster_heads': cluster_heads,
            'remaining_energy': remaining_energy,
            'packets_sent': packets_sent,
            'packets_received': packets_received,
            'energy_consumed': round_energy_consumed,
            'pdr': packets_received / packets_sent if packets_sent > 0 else 0,
            # RLç‰¹å®šæŒ‡æ ‡
            'total_reward': total_reward,
            'average_epsilon': avg_epsilon,
            'learning_enabled': self.learning_enabled
        }
        
        self.round_statistics.append(round_stats)
    
    def run_simulation(self, max_rounds: int) -> Dict[str, Any]:
        """è¿è¡ŒRLå¢å¼ºçš„EEHFRä»¿çœŸ"""
        
        print(f"ğŸ¤– å¼€å§‹å¼ºåŒ–å­¦ä¹ å¢å¼ºçš„Enhanced EEHFRåè®®ä»¿çœŸ")
        print(f"   èŠ‚ç‚¹æ•°é‡: {len(self.nodes)}")
        print(f"   æœ€å¤§è½®æ•°: {max_rounds}")
        print(f"   æ¢ç´¢è½®æ•°: {self.exploration_rounds}")
        
        start_time = time.time()
        
        for round_num in range(max_rounds):
            self.current_round = round_num
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å­˜æ´»èŠ‚ç‚¹
            alive_nodes = [node for node in self.nodes if node.is_alive]
            if len(alive_nodes) < 2:
                print(f"ğŸ’€ ç½‘ç»œåœ¨ç¬¬ {round_num} è½®ç»“æŸç”Ÿå‘½å‘¨æœŸ")
                break
            
            # å®šæœŸé‡æ–°é€‰æ‹©ç°‡å¤´
            if round_num % 20 == 0:
                self._select_cluster_heads()
                self._form_clusters()
            
            # è°ƒæ•´å­¦ä¹ å‚æ•°
            if round_num == self.exploration_rounds:
                print(f"ğŸ¯ ç¬¬ {round_num} è½®ï¼šåˆ‡æ¢åˆ°åˆ©ç”¨æ¨¡å¼")
                for agent in self.rl_agents.values():
                    agent.epsilon = 0.1
            
            # æ‰§è¡Œæ•°æ®æ”¶é›†é˜¶æ®µ
            collection_results = self._data_collection_phase()
            
            # æ‰§è¡Œæ•°æ®è½¬å‘é˜¶æ®µ
            forwarding_results = self._data_forwarding_phase()
            
            # åˆå¹¶ä¼ è¾“ç»“æœ
            all_results = collection_results + forwarding_results
            
            # æ›´æ–°RLæ™ºèƒ½ä½“
            self._update_rl_agents(all_results)
            
            # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
            self._collect_round_statistics(round_num, all_results)
            
            # æ›´æ–°æ€»ç»Ÿè®¡
            self.total_packets_sent += len(all_results)
            self.total_packets_received += sum(1 for r in all_results if r['success'])
            self.total_energy_consumed += sum(r['energy_consumed'] for r in all_results)
            
            # å®šæœŸè¾“å‡ºè¿›åº¦
            if round_num % 50 == 0:
                avg_reward = sum(sum(node.reward_history) for node in self.nodes if node.reward_history) / len(alive_nodes)
                print(f"   è½®æ•° {round_num}: å­˜æ´»èŠ‚ç‚¹ {len(alive_nodes)}, å¹³å‡å¥–åŠ± {avg_reward:.2f}")
        
        execution_time = time.time() - start_time
        
        # ç”Ÿæˆæœ€ç»ˆç»“æœ
        return self._generate_final_results(execution_time)
    
    def _generate_final_results(self, execution_time: float) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆç»“æœ"""
        
        final_alive_nodes = sum(1 for node in self.nodes if node.is_alive)
        network_lifetime = len(self.round_statistics)
        
        if self.total_energy_consumed > 0:
            energy_efficiency = self.total_packets_received / self.total_energy_consumed
        else:
            energy_efficiency = 0
        
        if self.total_packets_sent > 0:
            packet_delivery_ratio = self.total_packets_received / self.total_packets_sent
        else:
            packet_delivery_ratio = 0
        
        # RLå­¦ä¹ ç»Ÿè®¡
        total_successful = sum(node.successful_transmissions for node in self.nodes)
        total_failed = sum(node.failed_transmissions for node in self.nodes)
        success_rate = total_successful / (total_successful + total_failed) if (total_successful + total_failed) > 0 else 0
        
        # æ™ºèƒ½ä½“å­¦ä¹ ç»Ÿè®¡
        agent_stats = {}
        for node_id, agent in self.rl_agents.items():
            agent_stats[node_id] = agent.get_learning_stats()
        
        print(f"âœ… RLå¢å¼ºä»¿çœŸå®Œæˆï¼Œç½‘ç»œåœ¨ {network_lifetime} è½®åç»“æŸ")
        print(f"   å­¦ä¹ æˆåŠŸç‡: {success_rate:.3f}")
        print(f"   å¹³å‡Qè¡¨å¤§å°: {np.mean([stats['q_table_size'] for stats in agent_stats.values()]):.1f}")
        
        return {
            'protocol': 'RL_Enhanced_EEHFR',
            'network_lifetime': network_lifetime,
            'total_energy_consumed': self.total_energy_consumed,
            'final_alive_nodes': final_alive_nodes,
            'energy_efficiency': energy_efficiency,
            'packet_delivery_ratio': packet_delivery_ratio,
            'execution_time': execution_time,
            'round_statistics': self.round_statistics,
            'config': {
                'num_nodes': len(self.nodes),
                'initial_energy': self.config.initial_energy,
                'area_size': f"{self.config.area_width}x{self.config.area_height}",
                'packet_size': self.config.packet_size,
                'exploration_rounds': self.exploration_rounds
            },
            'rl_metrics': {
                'learning_success_rate': success_rate,
                'agent_statistics': agent_stats,
                'total_successful_transmissions': total_successful,
                'total_failed_transmissions': total_failed,
                'average_q_table_size': np.mean([stats['q_table_size'] for stats in agent_stats.values()]),
                'final_epsilon': np.mean([agent.epsilon for agent in self.rl_agents.values()])
            },
            'additional_metrics': {
                'total_packets_sent': self.total_packets_sent,
                'total_packets_received': self.total_packets_received,
                'average_cluster_heads': sum(stats['cluster_heads'] for stats in self.round_statistics) / len(self.round_statistics) if self.round_statistics else 0
            }
        }


# æµ‹è¯•å‡½æ•°
def test_rl_enhanced_eehfr():
    """æµ‹è¯•å¼ºåŒ–å­¦ä¹ å¢å¼ºçš„EEHFRåè®®"""
    
    print("ğŸ¤– æµ‹è¯•å¼ºåŒ–å­¦ä¹ å¢å¼ºçš„Enhanced EEHFRåè®®")
    print("=" * 70)
    
    # ç½‘ç»œé…ç½®
    config = NetworkConfig(
        num_nodes=50,
        area_width=100,
        area_height=100,
        initial_energy=2.0,
        base_station_x=50,
        base_station_y=50,
        packet_size=512
    )
    
    # åˆ›å»ºåè®®å®ä¾‹
    protocol = RLEnhancedEEHFRProtocol(config)
    
    # è¿è¡Œä»¿çœŸ
    result = protocol.run_simulation(200)
    
    # è¾“å‡ºç»“æœ
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"   ç½‘ç»œç”Ÿå­˜æ—¶é—´: {result['network_lifetime']} è½®")
    print(f"   æ€»èƒ½è€—: {result['total_energy_consumed']:.3f} J")
    print(f"   èƒ½æ•ˆ: {result['energy_efficiency']:.1f} packets/J")
    print(f"   æ•°æ®åŒ…æŠ•é€’ç‡: {result['packet_delivery_ratio']:.3f}")
    print(f"   å­¦ä¹ æˆåŠŸç‡: {result['rl_metrics']['learning_success_rate']:.3f}")
    print(f"   å¹³å‡Qè¡¨å¤§å°: {result['rl_metrics']['average_q_table_size']:.1f}")
    print(f"   æœ€ç»ˆÎµå€¼: {result['rl_metrics']['final_epsilon']:.3f}")
    
    return result


if __name__ == "__main__":
    test_rl_enhanced_eehfr()
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼ºåŒ–å­¦ä¹ å¢å¼ºçš„Enhanced EEHFRåè®® (RL-Enhanced EEHFR)

åŸºäºQ-Learningçš„è‡ªé€‚åº”è·¯ç”±å†³ç­–ï¼š
1. æ™ºèƒ½ä¸‹ä¸€è·³é€‰æ‹©
2. è‡ªé€‚åº”ç¯å¢ƒå­¦ä¹ 
3. å¤šç›®